{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CENTRO_OESTE2019ID.dta...\n",
      "Saved D:\\trab\\2019_cpf\\CENTRO_OESTE2019ID.dta.pkl\n",
      "Processing MG_ES_RJ2019ID.dta...\n",
      "Saved D:\\trab\\2019_cpf\\MG_ES_RJ2019ID.dta.pkl\n",
      "Processing NORDESTE2019ID.dta...\n",
      "Saved D:\\trab\\2019_cpf\\NORDESTE2019ID.dta.pkl\n",
      "Processing NORTE2019ID.dta...\n",
      "Saved D:\\trab\\2019_cpf\\NORTE2019ID.dta.pkl\n",
      "Processing SP2019ID.dta...\n",
      "Saved D:\\trab\\2019_cpf\\SP2019ID.dta.pkl\n",
      "Processing SUL2019ID.dta...\n",
      "Saved D:\\trab\\2019_cpf\\SUL2019ID.dta.pkl\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "input_dir = \"D:\\\\trab\\\\2019\"  # Change this to the actual path\n",
    "output_dir = \"D:\\\\trab\\\\2019_cpf\"  # Change this to where you want to save the .pkl files\n",
    "\n",
    "\n",
    "dta_files = [f for f in os.listdir(input_dir) if f.endswith(\".dta\")]\n",
    "\n",
    "# Process each .dta file\n",
    "for file in dta_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    output_path = os.path.join(output_dir, f\"{file}.pkl\")\n",
    "\n",
    "    print(f\"Processing {file}...\")\n",
    "\n",
    "    # Read file in chunks to handle large datasets\n",
    "    cpf_list = []  # List to store CPF column data\n",
    "\n",
    "    try:\n",
    "        iter_df = pd.read_stata(file_path, chunksize=100000)  # Adjust chunk size if needed\n",
    "\n",
    "        for chunk in iter_df:\n",
    "            if \"CPF\" in chunk.columns:  # Ensure the CPF column exists\n",
    "                cpf_list.append(chunk[[\"CPF\",\n",
    "                                       \"identificad\",\n",
    "                                       \"radiccnpj\",\n",
    "                                       \"dtadmissao\",\n",
    "                                       \"diadesli\",\n",
    "                                       \"mesdesli\",\n",
    "                                       \"remmedr\",\n",
    "                                       \"ocup2002\"]])\n",
    "\n",
    "        # Concatenate all chunks and save to .pkl\n",
    "        if cpf_list:\n",
    "            cpf_df = pd.concat(cpf_list, ignore_index=True)\n",
    "            cpf_df.to_pickle(output_path)\n",
    "            print(f\"Saved {output_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: CPF column not found in {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "input_dir = \"D:\\\\trab\\\\2020\"  # Change this to the actual path\n",
    "\n",
    "file=\"SP2020ID.dta\"\n",
    "file_path = os.path.join(input_dir, file)\n",
    "\n",
    "\n",
    "iter_df = pd.read_stata(file_path, chunksize=100000)  # Adjust chunk size if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erugg\\AppData\\Local\\Temp\\ipykernel_22080\\2089852943.py:2: CategoricalConversionWarning: \n",
      "One or more series with value labels are not fully labeled. Reading this\n",
      "dataset with an iterator results in categorical variable with different\n",
      "categories. This occurs since it is not possible to know all possible values\n",
      "until the entire dataset has been read. To avoid this warning, you can either\n",
      "read dataset without an iterator, or manually convert categorical data by\n",
      "``convert_categoricals`` to False and then accessing the variable labels\n",
      "through the value_labels method of the reader.\n",
      "\n",
      "  first_chunk = next(iter_df)\n",
      "C:\\Users\\erugg\\AppData\\Local\\Temp\\ipykernel_22080\\2089852943.py:2: CategoricalConversionWarning: \n",
      "One or more series with value labels are not fully labeled. Reading this\n",
      "dataset with an iterator results in categorical variable with different\n",
      "categories. This occurs since it is not possible to know all possible values\n",
      "until the entire dataset has been read. To avoid this warning, you can either\n",
      "read dataset without an iterator, or manually convert categorical data by\n",
      "``convert_categoricals`` to False and then accessing the variable labels\n",
      "through the value_labels method of the reader.\n",
      "\n",
      "  first_chunk = next(iter_df)\n",
      "C:\\Users\\erugg\\AppData\\Local\\Temp\\ipykernel_22080\\2089852943.py:2: CategoricalConversionWarning: \n",
      "One or more series with value labels are not fully labeled. Reading this\n",
      "dataset with an iterator results in categorical variable with different\n",
      "categories. This occurs since it is not possible to know all possible values\n",
      "until the entire dataset has been read. To avoid this warning, you can either\n",
      "read dataset without an iterator, or manually convert categorical data by\n",
      "``convert_categoricals`` to False and then accessing the variable labels\n",
      "through the value_labels method of the reader.\n",
      "\n",
      "  first_chunk = next(iter_df)\n",
      "C:\\Users\\erugg\\AppData\\Local\\Temp\\ipykernel_22080\\2089852943.py:2: CategoricalConversionWarning: \n",
      "One or more series with value labels are not fully labeled. Reading this\n",
      "dataset with an iterator results in categorical variable with different\n",
      "categories. This occurs since it is not possible to know all possible values\n",
      "until the entire dataset has been read. To avoid this warning, you can either\n",
      "read dataset without an iterator, or manually convert categorical data by\n",
      "``convert_categoricals`` to False and then accessing the variable labels\n",
      "through the value_labels method of the reader.\n",
      "\n",
      "  first_chunk = next(iter_df)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PIS', 'CPF', 'numectps', 'nome', 'identificad', 'radiccnpj',\n",
       "       'municipio', 'tpvinculo', 'empem3112', 'tipoadm', 'dtadmissao',\n",
       "       'causadesli', 'mesdesli', 'diadesli', 'ocupacao94', 'ocup2002',\n",
       "       'grinstrucao', 'genero', 'dtnascimento', 'idade', 'nacionalidad',\n",
       "       'portdefic', 'tpdefic', 'raca_cor', 'remdezembro', 'remmedia',\n",
       "       'remdezr', 'remmedr', 'tempempr', 'tiposal', 'salcontr', 'ultrem',\n",
       "       'horascontr', 'sbclas20', 'clascnae95', 'clascnae20', 'tamestab',\n",
       "       'natjuridica', 'tipoestbl', 'indceivinc', 'ceivinc', 'indalvara',\n",
       "       'indpat', 'indsimples', 'qtdiasafas', 'ibgesubsetor', 'anochegbr',\n",
       "       'cepestab', 'muntrab', 'razaosocial', 'indtrabparc', 'indtrabint',\n",
       "       'remjan', 'remfev', 'remmar', 'remabr', 'remmai', 'remjun', 'remjul',\n",
       "       'remago', 'remset', 'remout', 'remnov', 'causafast1', 'causafast2',\n",
       "       'causafast3', 'diainiaf1', 'diainiaf2', 'diainiaf3', 'diafimaf1',\n",
       "       'diafimaf2', 'diafimaf3', 'mesiniaf1', 'mesiniaf2', 'mesiniaf3',\n",
       "       'mesfimaf1', 'mesfimaf2', 'mesfimaf3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first chunk\n",
    "first_chunk = next(iter_df)\n",
    "\n",
    "# Display the first few rows of the chunk\n",
    "first_chunk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CENTRO_OESTE2020ID.dta...\n",
      "Saved D:\\trab\\2020_income\\CENTRO_OESTE2020ID.dta.pkl\n",
      "Processing MG_ES_RJ2020ID.dta...\n",
      "Saved D:\\trab\\2020_income\\MG_ES_RJ2020ID.dta.pkl\n",
      "Processing NORDESTE2020ID.dta...\n",
      "Saved D:\\trab\\2020_income\\NORDESTE2020ID.dta.pkl\n",
      "Processing NORTE2020ID.dta...\n",
      "Saved D:\\trab\\2020_income\\NORTE2020ID.dta.pkl\n",
      "Processing SP2020ID.dta...\n",
      "Saved D:\\trab\\2020_income\\SP2020ID.dta.pkl\n",
      "Processing SUL2020ID.dta...\n",
      "Saved D:\\trab\\2020_income\\SUL2020ID.dta.pkl\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "input_dir = \"D:\\\\trab\\\\2020\"  # Change this to the actual path\n",
    "output_dir = \"D:\\\\trab\\\\2020_income\"  # Change this to where you want to save the .pkl files\n",
    "\n",
    "\n",
    "dta_files = [f for f in os.listdir(input_dir) if f.endswith(\".dta\")]\n",
    "\n",
    "# Process each .dta file\n",
    "for file in dta_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    output_path = os.path.join(output_dir, f\"{file}.pkl\")\n",
    "\n",
    "    print(f\"Processing {file}...\")\n",
    "\n",
    "    # Read file in chunks to handle large datasets\n",
    "    cpf_list = []  # List to store CPF column data\n",
    "\n",
    "    try:\n",
    "        iter_df = pd.read_stata(file_path, chunksize=100000)  # Adjust chunk size if needed\n",
    "\n",
    "        for chunk in iter_df:\n",
    "            if \"CPF\" in chunk.columns:  # Ensure the CPF column exists\n",
    "                cpf_list.append(chunk[[\"CPF\",\n",
    "                                       \"remmedr\"]])\n",
    "\n",
    "        # Concatenate all chunks and save to .pkl\n",
    "        if cpf_list:\n",
    "            cpf_df = pd.concat(cpf_list, ignore_index=True)\n",
    "            cpf_df.to_pickle(output_path)\n",
    "            print(f\"Saved {output_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: CPF column not found in {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
